#Import Libraries
import random, torch, norm
import numpy as np
from model import Actor, Critic
from torch import optim

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#Agent Class
class Agent():
    """Interacts with and learns from the environment."""
    def __init__(self, state_size, action_size, learning_rate, actorFCunits, criticFCunits, seed):
        """Initialize an Agent object.
        
        Params
        ======
            state_size (int): dimension of each state
            action_size (int): dimension of each action
            seed (int): random seed
        """
        self.state_size = state_size
        self.action_size = action_size
        self.seed = random.seed(seed)

        #Neural Networks
        self.actor = Actor(state_size, action_size, seed, fc1_units=actorFCunits[0], fc2_units=actorFCunits[1]).to(device)
        self.actor_old = Actor(state_size, action_size, seed, fc1_units=actorFCunits[0], fc2_units=actorFCunits[1]).to(device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic = Critic(state_size, action_size, seed, fc1_units=criticFCunits[0], fc2_units=criticFCunits[1]).to(device)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)

    def act(self, state):
        """Returns actions for given state as per current policy.
        
        Params
        ======
            state (array_like): current state
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)            # convert state vector to torch format
        self.actor.eval()                                                          # put actor in evaluation mode
        with torch.no_grad():                                                      # turn off gradient computation
            action_values = self.actor(state)                                      # get action values from policy
        self.actor.train()                                                         # put actor in training mode
        action_values = action_values.numpy()
        # Probabilistic Action Selection
        actionsarray = np.array([]).reshape([0,action_values[0].shape[1]])
        for output in action_values[0]:
            actions = np.array([])
            for index in range(self.action_size):
                actions = np.append(actions,np.random.normal(output[2*index],output[2*index+1]))   # determine actions based on policy action values of mean and standard deviation
            actionsarray = np.vstack(actions)
        return actionsarray
    
    def policy_ratios(self, policy_prime, policy_old, states, actions):
        """Returns ratios of probabilities for the new and old policies to return the actions taken for a given state
        
        Params
        ======
            policy_prime (Agent.actor or Agent.critic): neural network new weights
            policy_old (Agent.actor or Agent.critic): neural network old weights
            states (array-like): history of the environment states from which the agent selected it's actions
            actions (array-like): history of the actions selected by the agent
        """
        probs_prime = np.array([]).reshape(0,self.action_size)                           # initialize trajectory action proabilities array
        probs_old = probs_prime
        for policy, probs in zip([policy_prime, policy_old],[probs_prime, probs_old]):
            for states, actions in zip(states,actions):
                policy.eval()                                                          # put actor in evaluation mode
                with torch.no_grad():                                                  # turn off gradient computation
                    action_values = policy(states)                                     # get action values from policy
                policy.train()                                                         # put actor in training mode
                means = action_values[0::2]                                            # extract probabilistic action distribution means
                sds = action_values[1::2]                                              # extract probabilistic action distribution standard deviations
                prob= []                                                               # intialize empty actions probabilities list
                for mean,sd,action in zip(means,sds,actions):
                    prob.append(norm.pdf(action, loc=mean, scale=sd))                  # calculate the probaility from the PDF of selecting the action
                probs.stack(prob)
        return np.divide(probs_prime, probs_old)
    
    def advantage(self, states, actions, lambd, gamma):
        """Returns the advantage estimate for each timestep in a trajectory generated by an agent/environment interaction episode.
        
        Params
        ======
            states (array-like): history of states for trajectory segment
            actions (array-like): history of actions for trajectory segment
            rewards (array_like): rewards obtained at each time step
            lmabd (float): advantage function discount factor
            gamma (float): discount factor applied to future rewards at each timestep
        """
        advantages = np.array([])
        deltas = []
        rt = self.policy_ratios(self.actor, self.actor_old, states, actions)            # calculate the iterative policy probability ratio for choosing these actions from these states
        for index, state in enumerate(states):
            if index < states.shape[0]:                                                 # exclude last index, since there is no next state
                deltas.append(rt[index,:,:] + gamma*self.critic(states[index+1,:,:]) - self.critic(state)) # calculate the delta terms at each iteration
                advantages[index,:,:] = np.multiply(deltas,[(lambd*gamma)**n for n in range(len(states)-1)]).sum # calculate advantage estimates at each iteration
        return advantages

    def clipped_surrogate(self, states, actions, advantages, epsilon=0.1):
        """Returns the clipped surrogate function comparing a new policy to an old one
        
        Params
        ======
            states (array-like): history of the environment states from which the agent selected it's actions
            actions (array-like): history of the actions selected by the agent
            advantages (array-like): advantage estimates at each time step
            epsilon (float): clipping limit for surrogate function
        """
        Lclip = np.zeros(1,self.action_size)
        ratios = self.policy_ratios(self.actor, self.actor_old, states, actions)        # calculate probability ratios of current and old policies to take the actions at the given states
        for ratio, advantage in zip(ratios, advantages):
            Lclip.dstack((Lclip,[min(ratio[index]*advantage[index], max(1-epsilon,min(ratio[index],1+epsilon))*advantage[index]) for index in len(ratio)]))
        return Lclip[:,:,1:].reshape(1,self.action_size,len(ratios))

    def expectation_eval(self, states, rewards):
        pass
        #TODO: Implement value function vs. future reward comparison loss function


    def optimize(self, states, actions, rewards, advantages, epsilon, minibatch_size, optimization_epochs):
        """Update actor and critic weights using the clipped surrogate function

        Params
        ======
           states (array-like): history of the environment states from which the agent selected it's actions
           actions (array-like): history of the actions selected by the agent
           advantages (array-like): advantage estimates at each time step
           epsilon (float): surrogate function clipping limit 
        """
        for epoch in range(optimization_epochs):
            #TODO: Implement minibatch sampling for actor/critic optimization
            #TODO: Implement policy and value network optimization
            self.actor.compile(optimizer=self.actor_optimizer, loss=-self.clipped_surrogate(states, actions, advantages, epsilon))
            self.critic.compile(optimizer=self.critic_optimizer, loss=-self.expectation_eval(states, rewards))
        return 0
        